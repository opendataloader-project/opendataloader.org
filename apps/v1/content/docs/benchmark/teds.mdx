---
title: Table Structure (TEDS)
description: Measures whether tables are accurately reconstructed
---

## Why Table Extraction Matters for RAG

Tables contain structured data that LLMs need to answer questions like "What was Q3 revenue?" or "Compare Product A vs B." If rows and columns are scrambled or merged incorrectly, the LLM gets wrong data and gives wrong answers.

**Example problem:** A financial table where cell values shift to wrong columns, causing the LLM to report incorrect figures.

## What TEDS Measures

TEDS (Tree Edit Distance Similarity) compares the structure of extracted tables against ground truth. A score of 1.0 means perfect reconstruction; lower scores indicate missing rows, merged cells, or scrambled content.

![Table structure](/figures/benchmark_table-structure.png)

## Results

| Engine         | Score | Rank |
|----------------|-------|------|
| Docling        | 0.89  | #1   |
| OpenDataLoader | 0.49  | #2   |
| PyMuPDF4LLM    | 0.40  | #3   |
| MarkItDown     | 0.00  | #4   |

- Table extraction remains the hardest problem â€” even the best engine scores below 0.90
- Borderless tables, nested headers, and merged cells cause errors across all engines

## When to Prioritize This Metric

| Use Case                           | Recommended Engine   |
|------------------------------------|----------------------|
| Financial documents with tables    | **Docling**          |
| Technical specs, comparison tables | **Docling**          |
| Simple bordered tables             | OpenDataLoader       |
| No tables in documents             | Any engine works     |

## Current Limitations

If your documents are table-heavy, test with your actual files before choosing an engine. Consider post-processing or manual review for critical data.

## Learn More

For detailed methodology, raw data, and reproduction scripts, see the [opendataloader-bench repository](https://github.com/opendataloader-project/opendataloader-bench).

<br />
<br />
